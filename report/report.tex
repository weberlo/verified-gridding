\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{doobs}
\usepackage{mathpartir}
\usepackage{minted}
\usepackage{titlesec}
\usepackage{tcolorbox}

\DeclareUnicodeCharacter{3B4}{$\bm{\delta}$}
\DeclareUnicodeCharacter{3B8}{$\bm{\theta}$}
\DeclareUnicodeCharacter{3BB}{$\lambda$}
\DeclareUnicodeCharacter{3BC}{$\mu$}
\DeclareUnicodeCharacter{3BC}{$\pi$}
\DeclareUnicodeCharacter{3C3}{$\sigma$}
\DeclareUnicodeCharacter{3C0}{$\pi$}
\DeclareUnicodeCharacter{2200}{$\forall$}
\DeclareUnicodeCharacter{2203}{$\exists$}
\DeclareUnicodeCharacter{2208}{$\in$}
\DeclareUnicodeCharacter{2208}{$\in$}
\DeclareUnicodeCharacter{2260}{$\neq$}
\DeclareUnicodeCharacter{2227}{$\wedge$}
\DeclareUnicodeCharacter{2225}{$\|$}
\DeclareUnicodeCharacter{2194}{$\iff$}
\DeclareUnicodeCharacter{00AC}{$\neg$}
\DeclareUnicodeCharacter{2228}{$\vee$}
\DeclareUnicodeCharacter{2115}{$\mathbb{N}$}
\DeclareUnicodeCharacter{2124}{$\mathbb{Z}$}
\DeclareUnicodeCharacter{207A}{$^+$}
\DeclareUnicodeCharacter{2264}{$\leq$}
\DeclareUnicodeCharacter{2192}{$\to$}


\titleformat{\section}
  {\normalfont\Large\bfseries}{\thesection}{1em}{}[{\titlerule[0.3pt]}]

\begin{document}

\title{%
  Final Project Report
  \\[0.5em]
  \large Geometric Computing \\
}

\author{
Logan Weber\\
loganweb@mit.edu
}

\def\aSigma{\overline{\Sigma}}
\def\asigma{\overline{\sigma}}
\def\aheap{\overline{h}}

\date{}

\maketitle

\section{TODOs}
Recall, Piotyr said what's most useful for him is statistics (breakdowns of code vs. proof) and ``war stories''.

\section{Introduction}
In the past few years, the Lean theorem prover and programming language has caught the attention of the math community.
For a long time, proof assistants were thought of as unsatisfactory for formalizing undergraduate-level mathematics, let alone \textit{research}-level mathematics.
This image began to change when a challenge by accomplished mathematician, Peter Scholze, was proposed and taken up by the Lean community.
This challenge became known as \href{https://xenaproject.wordpress.com/2020/12/05/liquid-tensor-experiment/}{The Liquid Tensor Experiment}.


% No single innovation
The rise of Lean cannot be explained by any significant \textit{technological} innovation---it was a \textit{cultural} innovation.
% TODO this makes it sound like Lean isn't sound
The most relevant comparison is to the Coq proof assistant, which, in contrast to Lean, has emphasized sound, type-theoretic principles from its conception.
Lean has instead focused on working backwards from how mathematicians prefer to work.
As a result, Lean has incorporated quotient types and embraces classical mathematics, which uses the law of excluded middle, destroying computational properties.
With that being said.


% TODO doesn't seem like this is true after reading into it. seems more like the emphasis on the axiom of choice and baked-in quotient types was important.
It was a plethora of performance engineering and ergonomic improvements that made Lean an attractive platform for the formalization of mathematics.

In one of the earlier lectures, a student asked Professor Indyk how some of algorithm analysis we had done could be made formal.
He replied with an anecdote that a student once tried to formally verify a sweep algorithm in his class.
The student restricted to integers, then to 1 dimension, and ended up proving that you could sort a list of integers.
The author took this as a challenge and decided to assess the current state of formal verification, given the existence of libraries like Mathlib.

\section{The Problem Specification}

\begin{minted}{lean}
def closest_pair (p q : point) (ps : list point) : Prop :=
  (p ∈ ps) ∧
  (q ∈ ps) ∧
  (p ≠ q) ∧
  (∀ (r s : point),
      (same_pair (p, q) (r, s) ↔ ∥ p - q ∥ = ∥ r - s ∥) ∧
     (¬(same_pair (p, q) (r, s)) ↔ ∥ p - q ∥ < ∥ r - s ∥))

def closest_pair_with_help (p q : point) (ps : list point) (c : ℕ⁺) : Prop :=
  (closest_pair p q ps) ∧ (1 < ∥ p - q ∥) ∧ (∥ p - q ∥ ≤ c)
\end{minted}
\newpage


\begin{minted}{lean}
def find_closest_pair (ps : list point) (c : ℕ⁺) : (point × point) := sorry

theorem find_closest_pair_correct :
  ∀ (ps : list point) (c : ℕ⁺),
    (∃ (p q : point),
      (closest_pair p q ps) ∧ (1 < ∥ p - q ∥) ∧ (∥ p - q ∥ ≤ c)) →
    (∃ (p q : point),
      find_closest_pair ps c = (p, q) ∧
      closest_pair p q ps) := begin
  sorry
end
\end{minted}


\section{Integer Grids}
\begin{align*}
  \| (k - 1, k-1) \|_2^2 &\leq 1 \\
  2(k-1)^2 & \leq 1 \\
  % (k-1)^2 + (k-1)^2 & \leq 1 \\
  % 2(k^2 - 2k + 1) &\leq 1 \\
  % \frac{1}{2}(2(k^2 - 2k + 1)) &\leq \frac{1}{2} \\
  % k^2 - 2k + 1 &\leq 0
\end{align*}
The only integral solution is 1.

Solution: $k=1$

\section{Strengthening the Induction Hypothesis}
Our core algorithm recurses on the list of points, combining results from subcalls to eventually arrive at the closest pair.
Then it seems natural for our proof of the algorithm's correctness to mirror its definition, which we could achieve by inducting on the list of points.

However, inducting on the list of points to prove the closest pair predicate gives us an incorrect induction hypothesis.
The induction hypothesis will state that the recursive call contains the closest pair among all of the points processed in the recursive call.
But this is not true!
Since we draw balls around each point that we process, we may pick points outside of the set of points we have recursed over.

Thus, we must strengthen the induction hypothesis to more closely align our analysis of the algorithm than the definition.

\begin{minted}{lean}
inductive closest_pair_in_balls (c : ℕ⁺) (qs : list point) :
  option (point × point) → list point → Prop
| no_ball (xy : option (point × point)) : closest_pair_in_balls xy []
| succ_ball (xy : option (point × point)) (p : point) (ps' : list point) :
    ((∃ (q : point), q ∈ qs ∧ ∥q - p∥ ≤ c) →
      ∃ (x y : point),
        xy = some (x, y) ∧
        (∀ (q : point),
            q ∈ qs →
            ∥q - p∥ ≤ c →
            ∥x - y∥ ≤ ∥q - p∥)) →
    closest_pair_in_balls xy ps' →
    closest_pair_in_balls xy (p :: ps')
\end{minted}

\section{Admitted Lemmas}
\begin{minted}{lean}
lemma not_x_lt_y_and_gt_y :
  ∀ (x y : ℤ),
    ¬(x < y ∧ y < x) := begin
  sorry
end

lemma not_lt_and_not_gt_implies_eq :
  ∀ (x y : ℤ), ¬(x < y) → ¬(y < x) → x = y := begin
  sorry
end

lemma int_nonneg_add :
  ∀ (x y : ℤ), int.nonneg x → int.nonneg y → int.nonneg (x + y) := begin
    sorry
end

lemma int_x_times_x_nonneg :
  ∀ (x : ℤ), int.nonneg (x * x) := begin
    sorry
end

lemma nonneg_iff_leq_zero :
  ∀ (x : ℤ), int.nonneg x ↔ 0 ≤ x := begin
    sorry
  end
\end{minted}

\newpage
\begin{minted}{lean}
lemma a_nonneg_times_b_nonneq_means_a_times_b_nonneg :
  ∀ (a b : ℤ), int.nonneg a → int.nonneg b → int.nonneg (a * b) := begin
    sorry
end

lemma lift_nat_nonneg :
  ∀ (n : ℕ), int.nonneg ↑n := begin
    sorry
end

lemma x_div_plus_y_div_eq_x_plus_y_div_minus_sum_mod_div :
  ∀ (x y z : ℤ), x/z + y/z = (x + y)/z - (x % z + y % z) := begin
    sorry
  end

lemma x_div_y_times_y :
  ∀ (x y : ℤ), (x / y) * y = x - x % y := begin
    sorry
end

lemma point_norm_add_comm :
  ∀ (p q : point), ∥ p + q ∥ = ∥ q + p ∥ := begin
    sorry
end

lemma point_norm_sub_comm :
  ∀ (p q : point), ∥ p - q ∥ = ∥ q - p ∥ := begin
    sorry
end

lemma not_leq_and_gt : ∀ (p : point) (n : ℤ),
  ¬((∥p∥ ≤ n) ∧ (n < ∥p∥)) := begin
    sorry
end

lemma exists_closest_pair_implies_nonempty :
  ∀ (ps : list point), (∃ (p q : point), closest_pair p q ps) → ps ≠ [] :=
begin
  sorry
end

lemma exists_q_in_range_implies_aux_finds_it:
  ∀ (q : point) (qs' : list point) (ps : list point) (c : ℕ⁺),
    (∃ (p : point), p ∈ ps ∧ ∥p - q∥ ≤ ↑c) →
    (∃ (x y : point),
      aux (grid_points c ps) (q :: qs') = some (x, y) ∧
      ∀ (p : point),
        p ∈ ps →
        ∥p - q∥ ≤ ↑c →
        ∥x - y∥ ≤ ∥p - q∥) := begin
  sorry
end

lemma cp_with_help_implies_eps_separated :
  ∀ (p q : point) (ps : list point) (c : ℕ⁺),
    cp_with_help p q ps c → eps_separated ps 1 := begin
  sorry
end

lemma get_neighbs_contains_all_within_ball :
  ∀ (c : ℕ⁺) (ps : list point) (p q : point) (g : grid_2D),
    (∥ p - q ∥ ≤ c) → (q ∈ get_neighbs p g) := begin
  sorry
end

lemma get_min_dist_point_in_neighbs_correct :
  ∀ (p q : point) (g : grid_2D),
    ∥ p - q ∥ ≤ g.c →
      ∃ (x : point),
        get_min_dist_point_in_neighbs p g = some (x, p) ∧
        ∥ p - x ∥ ≤ ∥ p - q ∥ := begin
  sorry
end
\end{minted}

\subsection{Likely Existing in Mathlib.}
The author believes most admitted lemmas about the integers can be mapped onto existing proofs in Mathlib.

Examples:
\begin{itemize}
  \item An integer cannot be simultaneously smaller and larger than another integer.
  \item If an integer is not smaller or larger than another integer, then they are equal.
  \item Adding nonnegative integers is nonnegative.
\end{itemize}

\subsection{Not Existing in Mathlib}
\paragraph{Closest Pair.}
Examples:
\begin{itemize}
  \item If there's a closest pair, then the set of points is nonempty.
  \item If the closest pair is at least distance 1 apart, then all points are at least distance 1 apart.
  \item Finding all cells intersecting a hypercube of radius $\sqrt{c}$ finds all points in a ball of radius $c$.
  \item Given a list of points, we can find the closest point to a distinguished point.
\end{itemize}

\paragraph{Norm.}
% TODO we haven't `admit`ted everything in this section
% TODO need a different name than `point_norm'.
We can't directly leverage the infrastructure and theorems surrounding normed spaces, because the norm typeclass requires a codomain of $\R$, destroying computability properties.
Thus, we admit the following lemmas:
\begin{itemize}
  \item The point norm is commutative.
\end{itemize}
Thus, we prove the following lemmas:
\begin{itemize}
  \item The point norm is nonnegative.
\end{itemize}
In principle, a norm instance could be provided whereby the output would be lifted from $\Z$ to $\R$.
Then, perhaps we could transport theorems from normed spaces.
This might work, but it could also introduce a lot of overhead, where we intersperse coercions between $\Z$ and $\R$ throughout our proofs.

\section{Ergonomics}
Because proof assistants are so sensitive to the syntactic phrasing of concepts, it is common that creating new notation has dramatic impacts on the ergonomics of proving things.

\paragraph{Example.}
One bit of notation that has helped considerably is defining $\leq$ between point pairs that could be null.
Because the algorithm may return no pair---either when the distance hint is wrong or there are fewer than 2 points---the type signature is naturally expressed with an \textit{optional} return type.
In the algorithm, we want to know when the recursive result pair is closer than any pair found in the current point's ball.
Since either of these could actually be null, we have to case on their respective nullities.


\subsection{Decidable Propositions}
The expression $p \leq q$ is a proposition, so we may be able to \textit{prove} it true.
At the same time, for certain types of $p, q$ (e.g., $p, q \in \Z$), we have a decision procedure for the truth value of $p \leq q$, meaning we can \textit{program} with it in statements like $\text{if} p \leq q \text{then} 1 \text{else} 0$.
Lean claims to have facilities for dissolving this dichotomy, but they are not well documented and led me through a lot of guesswork about how to use them.
Ultimately, I made two syntactically identical definitions for $\mathsf{option} (\mathsf{point} \times \mathsf{point})$: one with a return type of $\mathsf{Prop}$ and one with a return type of $\mathsf{bool}$.

\subsection{Syntax Sensitivity}
The size of a proof depends a lot on how you've chosen to structure your code.
If one definition in your code uses $\| p - q \|$ and another uses $\| q - p \|$, and you're trying to relate them, you will need to rewrite $\| p - q \|$ into $\| q - p \|$ via commutativity.
Each mismatch of this sort in your code will result in an additional line in your proof, wherever the mismatched definitions need to be related.
Each such inconsistency compounds and causes quite a large blowup in the size of proofs.

There is technically automation that could make this a non-issue.
For lemmas like commutativity, that are a workhorse of many theorems, the simplification tactic, \texttt{simp}, can be augmented to apply this lemma automatically.
If the author learned how to use this automation sooner, perhaps some of this pain could have been avoided.

\subsection{The Reals}
One of the chief reasons we cannot answer that formal verification is easy is because of the complicated relationship modern proof assistants still have with the reals.
To work with the reals, all of your definitions become uncomputable, but this doesn't need to be the case.

\subsection{Rewriting Under Binders}
If you have a theorem that says $\forall x. 2x = x + x$, then you can't directly apply it to $\forall x. 2x + 2x$ to produce $\forall x. x + x + x + x$.
You need to start a subproof, introduce the variable $x$, apply the theorem, then close the subproof.

\section{Conclusion}
We are forced to the conclusion that the average programmer can continue to disregard formal verification, and formal verification should only be employed for the most critical components of digital infrastructure.

\bibliographystyle{plain}
\bibliography{references}
\end{document}
